program: src/train_git_base_osf_text.py
name: text_only_training_flamingo_seeds_1_and_2
method: grid
metric:
  goal: minimize
  name: val_loss
parameters:
  lr:
    values: [1e-5]
  batch_size:
    values: [256]
  n_epochs:
    values: [20]
  min_save_every:
    values: [1]
  n_workers:
    values: [20]
  optimizer:
    values: ["adam"]
  dataset_size:
    values: [-1]
  seed:
    values: [0]
  do_curriculum:
    values: [False]  # This is False for standard-finetuning.
  model_name:
    values: ['flamingo']  # Can be 'flamingo' or 'git'. NOTE: although the train_git_osf_text.py file is used, the --model_name can be Flamingo for flamingo training.
  model_type:
    values: ['causal_lm']  # Can be 'causal_lm' or 'sequence'.
  max_token_length:
    values: [50]
  initialize_with_text:
    values: [False]
  use_accelerate:
    values: [False]
  fp16:
    values: [True]
  tokenizer_path:
    values: ['./src/tokenizer/hf_wordpiece_tokenizer_from_bert-base-uncased/']
    # TODO: See if the same tokenizer can be used for Flamingo. The current version of the paper actually does use the same tokenizer.
  text_init_model_path:
    values: [None]
  load_optimizer:
    values: [False]
  train_on_full_data:
    values: [False]
  wandb_mode:
    values: ['online']